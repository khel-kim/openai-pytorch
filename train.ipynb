{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from openai.gpt1 import OpenAIGPTModel\n",
    "from utils import read_json, save_json, make_dot_dict\n",
    "from en_tokenizer import CharTokenizer\n",
    "from dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIGPTLMHeadModel(nn.Module):\n",
    "    def __init__(self, hp, pad_token_id):\n",
    "        super(OpenAIGPTLMHeadModel, self).__init__()\n",
    "        self.transformer = OpenAIGPTModel(hp, pad_token_id)\n",
    "        self.lm_head = nn.Linear(hp.d_model, hp.vocab_size, bias=False)\n",
    "        \n",
    "    def load_pretrained_model(self, path):\n",
    "        pass\n",
    "    \n",
    "    def save_weight(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(object):\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "    \n",
    "    def __call__(self, logits, target):\n",
    "        # logits : [batch, seq_len, vocab_size]\n",
    "        # target : [batch, seq_len]\n",
    "        vocab_size = logits.size()[-1]\n",
    "        \n",
    "        shift_logits = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n",
    "        shift_true = target[:, 1:].contiguous().view(-1)\n",
    "        padding_mask = (shift_true == self.pad_token_id).to(shift_logits.dtype)\n",
    "        \n",
    "        loss = F.cross_entropy(shift_logits, shift_true,\n",
    "                              reduction='none')\n",
    "        loss *= 1. - padding_mask\n",
    "        return loss.sum() / (1.-padding_mask).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAccuracy(object):\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "        \n",
    "    def __call__(self, logits, target):\n",
    "        vocab_size = logits.size()[-1]\n",
    "        \n",
    "        shift_logits = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n",
    "        shift_pred = shift_logits.argmax(dim=1)\n",
    "        \n",
    "        shift_true = target[:, 1:].contiguous().view(-1)\n",
    "        padding_mask = (shift_true == self.pad_token_id).float()\n",
    "        \n",
    "        tp = (shift_pred == shift_true).float()\n",
    "        tp *= 1. - padding_mask\n",
    "        return tp.sum() / (1. - padding_mask).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to(obj, device):\n",
    "    if torch.is_tensor(obj):\n",
    "        return obj.to(device)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: move_to(value, device) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [move_to(value, device) for value in obj]\n",
    "    else:\n",
    "        raise AssertionError()\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    val, avg, sum, count = [None] * 4\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        template = \"{} {:.3f} ({:.3f})\"\n",
    "        return template.format(self.name, self.val, self.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = read_json('config.json')\n",
    "hp = make_dot_dict(hp)\n",
    "\n",
    "tokenizer = CharTokenizer(model_path='sentencepiece_models/cp-char.model')\n",
    "dataset = CustomDataset(hp=hp, root='data/docs', tokenizer=tokenizer)\n",
    "\n",
    "hp.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "_lr = 1e-4\n",
    "_epochs = 2\n",
    "_batch_size = 2\n",
    "\n",
    "model = OpenAIGPTLMHeadModel(hp, tokenizer.pad_token_id)\n",
    "loss_fn = CustomLoss(tokenizer.pad_token_id)\n",
    "opt = torch.optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                       lr=_lr, \n",
    "                       betas=(0.9, 0.999),\n",
    "                       weight_decay=1e-4)\n",
    "\n",
    "metrics = {'acc' : CustomAccuracy(tokenizer.pad_token_id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAIGPTLMHeadModel(\n",
      "  (transformer): OpenAIGPTModel(\n",
      "    (embed): Embedding(\n",
      "      (words_embed): Embedding(47, 128)\n",
      "      (positions_embed): Embedding(512, 128)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): Block(\n",
      "        (attn): Attention(\n",
      "          (W_QKV): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Block(\n",
      "        (attn): Attention(\n",
      "          (W_QKV): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (proj_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=47, bias=False)\n",
      ")\n",
      "epochs : 0.054 loss : 2.505 acc : 0.232\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "print(model)\n",
    "for epoch in range(_epochs):\n",
    "    for x in dataset:\n",
    "        n_data = len(x)\n",
    "        \n",
    "        n_step = n_data // _batch_size\n",
    "        indices = np.arange(n_data)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for i in range(0, n_step, _batch_size):\n",
    "            c_idx = indices[i:i+_batch_size]\n",
    "            \n",
    "            c_x = x[c_idx]\n",
    "            lm_logits = model(c_x)\n",
    "            \n",
    "            loss = loss_fn(lm_logits, c_x)\n",
    "            m = {key: value(lm_logits, c_x) for key, value in metrics.items()}\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            print(f\"loss : {loss.item():.3f} \" \n",
    "                  + \" \".join([f\"{key} : {value.item():.3f}\" for key, value in m.items()])\n",
    "                 , end='\\r')\n",
    "#     print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, loss_fn, optimizer, metrics):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "        \n",
    "        self.train_eval = {\"train_loss\": AverageMeter(\"train_loss\")}\n",
    "        for key in metrics.keys():\n",
    "            self.train_eval[f\"train_{key}\"] = AverageMeter(f\"train_{key}\")\n",
    "\n",
    "        self.dev_eval = {\"dev_loss\": AverageMeter(\"dev_loss\")}\n",
    "        for key in metrics.keys():\n",
    "            self.dev_eval[f\"dev_{key}\"] = AverageMeter(f\"dev_{key}\")\n",
    "    \n",
    "    def fit(self, train_dataset, epochs, batch_size ,dev_dataset=None callbacks=None):\n",
    "        try:\n",
    "            xmp\n",
    "        except:\n",
    "            index = None\n",
    "            self.map_fn(index, train_dataset, epochs, batch_size, dev_dataset, callbacks)\n",
    "    \n",
    "    def map_fn(self, index, train_dataset, epochs, batch_size, dev_dataset=None, callbacks):\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p36",
   "language": "python",
   "name": "pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
